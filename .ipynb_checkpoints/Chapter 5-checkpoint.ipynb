{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / (np.sum(exps).reshape(-1,1))\n",
    "\n",
    "def relu(x):\n",
    "    return 1.0*(x>0)\n",
    "\n",
    "def leaky_relu(x, leaky_slope):\n",
    "    d=np.zeros_like(x)\n",
    "    d[x<=0]= leaky_slope\n",
    "    d[x>0]=1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Defining dummy values of x \n",
    "x = np.linspace(-np.pi, np.pi, 12)\n",
    "\n",
    "#Finding the Activation Function Outputs\n",
    "sigmoid_output = sigmoid(x)\n",
    "tanh_output = tanh(x)\n",
    "softmax_output = softmax(x)\n",
    "relu_output = relu(x)\n",
    "leaky_relu_output = leaky_relu(x,1)\n",
    "\n",
    "#Printing the Outputs\n",
    "print(sigmoid_output)\n",
    "print(tanh_output)\n",
    "print(softmax_output)\n",
    "print(relu_output)\n",
    "print(leaky_relu_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "f1 = plt.figure(1)\n",
    "plt.imshow(X_train[0])\n",
    "f2 = plt.figure(2)\n",
    "plt.imshow(X_train[1])\n",
    "plt.show()\n",
    "\n",
    "#check image shape and data count\n",
    "print(X_train[0].shape, len(X_train))\n",
    "print(X_train[0].shape, len(X_test))\n",
    "\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(len(X_train),28,28,1)\n",
    "X_test = X_test.reshape(len(X_test),28,28,1)\n",
    "\n",
    "#One-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train[0]\n",
    "\n",
    "#Create model\n",
    "model = Sequential()\n",
    "\n",
    "#Add Input CNN Layer\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "#Add second CNN Layer\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "#Add the fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "\n",
    "#predict first 6 images in the test set\n",
    "model.predict(X_test[:6])\n",
    "\n",
    "#actual results for first 6 images in the test set\n",
    "y_test[:6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "\n",
    "#Generating Random Data    \n",
    "t=np.arange(0,1000)\n",
    "x=np.sin(0.02*t)+2*np.random.rand(1000)\n",
    "df = pd.DataFrame(x)\n",
    "df.head()\n",
    "\n",
    "#Splitting into Train and Test set\n",
    "values=df.values\n",
    "train, test = values[0:800,:], values[800:1000,:]\n",
    "\n",
    "# convert dataset into matrix\n",
    "def convertToMatrix(data, step=4):\n",
    "    X, Y =[], []\n",
    "    for i in range(len(data)-step):\n",
    "        d=i+step  \n",
    "        X.append(data[i:d,])\n",
    "        Y.append(data[d,])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "trainX,trainY =convertToMatrix(train,6)\n",
    "testX,testY =convertToMatrix(test,6)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "#Making the RNN Structure\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=32, input_shape=(1,6), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\")) \n",
    "model.add(Dense(1))\n",
    "\n",
    "#Compiling the Code\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "#Training the Model\n",
    "model.fit(trainX,trainY, epochs=1, batch_size=500, verbose=2)\n",
    "\n",
    "#Predicting with the Model\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict= model.predict(testX)\n",
    "predicted=np.concatenate((trainPredict,testPredict),axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "#Generating Random Data    \n",
    "t=np.arange(0,1000)\n",
    "x=np.sin(0.02*t)+2*np.random.rand(1000)\n",
    "df = pd.DataFrame(x)\n",
    "df.head()\n",
    "\n",
    "#Splitting into Train and Test set\n",
    "values=df.values\n",
    "train, test = values[0:800,:], values[800:1000,:]\n",
    "\n",
    "# convert dataset into matrix\n",
    "def convertToMatrix(data, step=4):\n",
    "    X, Y =[], []\n",
    "    for i in range(len(data)-step):\n",
    "        d=i+step  \n",
    "        X.append(data[i:d,])\n",
    "        Y.append(data[d,])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "trainX,trainY =convertToMatrix(train,6)\n",
    "testX,testY =convertToMatrix(test,6)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "#Making the LSTM Structure\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=4, input_shape=(1,6), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\")) \n",
    "model.add(Dense(1))\n",
    "\n",
    "#Compiling the Code\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "#Training the Model\n",
    "model.fit(trainX,trainY, epochs=1, batch_size=500, verbose=2)\n",
    "\n",
    "#Predicting with the Model\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict= model.predict(testX)\n",
    "predicted=np.concatenate((trainPredict,testPredict),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated REcurrent Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    " \n",
    "#Generating Random Data    \n",
    "t=np.arange(0,1000)\n",
    "x=np.sin(0.02*t)+2*np.random.rand(1000)\n",
    "df = pd.DataFrame(x)\n",
    "df.head()\n",
    " \n",
    "#Splitting into Train and Test set\n",
    "values=df.values\n",
    "train, test = values[0:800,:], values[800:1000,:]\n",
    " \n",
    "# convert dataset into matrix\n",
    "def convertToMatrix(data, step=4):\n",
    "\tX, Y =[], []\n",
    "\tfor i in range(len(data)-step):\n",
    "    \td=i+step\n",
    "    \tX.append(data[i:d,])\n",
    "    \tY.append(data[d,])\n",
    "\treturn np.array(X), np.array(Y)\n",
    " \n",
    "trainX,trainY =convertToMatrix(train,6)\n",
    "testX,testY =convertToMatrix(test,6)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    " \n",
    "#Making the GRU Structure\n",
    "model = Sequential()\n",
    "model.add(GRU(units=4, input_shape=(1,6), activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    " \n",
    "#Compiling the Code\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()\n",
    " \n",
    "#Training the Model\n",
    "model.fit(trainX,trainY, epochs=10, batch_size=500, verbose=1)\n",
    " \n",
    "#Predicting with the Model\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict= model.predict(testX)\n",
    "predicted=np.concatenate((trainPredict,testPredict),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    " \n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-step-1):\n",
    "    \ta = dataset[i:(i+step), 0]\n",
    "    \tdataX.append(a)\n",
    "    \tdataY.append(dataset[i + step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "    \n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('carriage.csv', usecols=[1])\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    " \n",
    "# standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)\n",
    " \n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.90)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    " \n",
    "# Reshaping Data for the model\n",
    "step = 1\n",
    "train_X, train_Y = create_dataset(train, step)\n",
    "test_X, test_Y = create_dataset(test, step)\n",
    " \n",
    "train_X = numpy.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = numpy.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    " \n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(1, step)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()\n",
    "model.fit(train_X, train_Y, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "# create and fit the GRU network\n",
    "model1 = Sequential()\n",
    "model1.add(GRU(10, input_shape=(1, step)))\n",
    "model1.add(Dense(1))\n",
    "model1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model1.summary()\n",
    "model1.fit(train_X, train_Y, epochs=10, batch_size=50, verbose=1)\n",
    " \n",
    "# make predictions from LSTM\n",
    "trainPredict = model.predict(train_X)\n",
    "testPredict = model.predict(test_X)\n",
    "\n",
    "# make predictions from GRU\n",
    "trainPredict1 = model1.predict(train_X)\n",
    "testPredict1 = model1.predict(test_X)\n",
    " \n",
    "# invert predictions from LSTM\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "train_Y = scaler.inverse_transform([train_Y])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "test_Y = scaler.inverse_transform([test_Y])\n",
    "\n",
    "# invert predictions from GRU\n",
    "trainPredict1 = scaler.inverse_transform(trainPredict1)\n",
    "testPredict1 = scaler.inverse_transform(testPredict1)\n",
    " \n",
    "# calculate root mean squared error for LSTM\n",
    "print(\"*****Results for LSTMs*****\")\n",
    "trainScore = math.sqrt(mean_squared_error(train_Y[0], trainPredict[:,0]))\n",
    "print('Error in Training data is: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(test_Y[0], testPredict[:,0]))\n",
    "print('Error in Testing data is: %.2f RMSE' % (testScore))\n",
    "\n",
    "# calculate root mean squared error for GRU\n",
    "print(\"*****Results for GRUs*****\")\n",
    "trainScore1 = math.sqrt(mean_squared_error(train_Y[0], trainPredict1[:,0]))\n",
    "print('Error in Training data is: %.2f RMSE' % (trainScore1))\n",
    "testScore1 = math.sqrt(mean_squared_error(test_Y[0], testPredict1[:,0]))\n",
    "print('Error in Testing data is: %.2f RMSE' % (testScore1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADP",
   "language": "python",
   "name": "adp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
